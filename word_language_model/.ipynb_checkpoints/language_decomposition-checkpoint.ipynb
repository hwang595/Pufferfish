{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Sizes: W_ii torch.Size([1500, 1500]), W_hi: torch.Size([1500, 1500]), W_if: torch.Size([1500, 1500]), W_hf: torch.Size([1500, 1500]), W_ig: torch.Size([1500, 1500]), W_hg: torch.Size([1500, 1500]), W_io: torch.Size([1500, 1500]), W_ho: torch.Size([1500, 1500])\n",
      "##### Sizes: W_ii torch.Size([1500, 1500]), W_hi: torch.Size([1500, 1500]), W_if: torch.Size([1500, 1500]), W_hf: torch.Size([1500, 1500]), W_ig: torch.Size([1500, 1500]), W_hg: torch.Size([1500, 1500]), W_io: torch.Size([1500, 1500]), W_ho: torch.Size([1500, 1500])\n"
     ]
    }
   ],
   "source": [
    "vanilla_model = model.RNNModel(\"LSTM\", 33278, 1500, 1500, 2, dropout=0.65, tie_weights=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rank_model = model.LowRankRNNModel(\"LSTM\", 33278, 1500, 1500, 2, dropout=0.65, \n",
    "                            rank_ratio=0.25,\n",
    "                            tie_weights=True,\n",
    "                            ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param index: 0, name: encoder.weight param: torch.Size([33278, 1500])\n",
      "param index: 1, name: rnn.lstm1.W_ii param: torch.Size([1500, 1500])\n",
      "param index: 2, name: rnn.lstm1.W_hi param: torch.Size([1500, 1500])\n",
      "param index: 3, name: rnn.lstm1.b_i param: torch.Size([1500])\n",
      "param index: 4, name: rnn.lstm1.W_if param: torch.Size([1500, 1500])\n",
      "param index: 5, name: rnn.lstm1.W_hf param: torch.Size([1500, 1500])\n",
      "param index: 6, name: rnn.lstm1.b_f param: torch.Size([1500])\n",
      "param index: 7, name: rnn.lstm1.W_ig param: torch.Size([1500, 1500])\n",
      "param index: 8, name: rnn.lstm1.W_hg param: torch.Size([1500, 1500])\n",
      "param index: 9, name: rnn.lstm1.b_g param: torch.Size([1500])\n",
      "param index: 10, name: rnn.lstm1.W_io param: torch.Size([1500, 1500])\n",
      "param index: 11, name: rnn.lstm1.W_ho param: torch.Size([1500, 1500])\n",
      "param index: 12, name: rnn.lstm1.b_o param: torch.Size([1500])\n",
      "param index: 13, name: rnn.lstm2.W_ii param: torch.Size([1500, 1500])\n",
      "param index: 14, name: rnn.lstm2.W_hi param: torch.Size([1500, 1500])\n",
      "param index: 15, name: rnn.lstm2.b_i param: torch.Size([1500])\n",
      "param index: 16, name: rnn.lstm2.W_if param: torch.Size([1500, 1500])\n",
      "param index: 17, name: rnn.lstm2.W_hf param: torch.Size([1500, 1500])\n",
      "param index: 18, name: rnn.lstm2.b_f param: torch.Size([1500])\n",
      "param index: 19, name: rnn.lstm2.W_ig param: torch.Size([1500, 1500])\n",
      "param index: 20, name: rnn.lstm2.W_hg param: torch.Size([1500, 1500])\n",
      "param index: 21, name: rnn.lstm2.b_g param: torch.Size([1500])\n",
      "param index: 22, name: rnn.lstm2.W_io param: torch.Size([1500, 1500])\n",
      "param index: 23, name: rnn.lstm2.W_ho param: torch.Size([1500, 1500])\n",
      "param index: 24, name: rnn.lstm2.b_o param: torch.Size([1500])\n",
      "param index: 25, name: decoder.bias param: torch.Size([33278])\n"
     ]
    }
   ],
   "source": [
    "for p_index, (name, param) in enumerate(vanilla_model.named_parameters()):\n",
    "    print(\"param index: {}, name: {} param: {}\".format(p_index, name, param.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param index: 0, name: encoder.weight param: torch.Size([33278, 1500])\n",
      "param index: 1, name: rnn.lstm1.W_ii param: torch.Size([1500, 1500])\n",
      "param index: 2, name: rnn.lstm1.W_hi param: torch.Size([1500, 1500])\n",
      "param index: 3, name: rnn.lstm1.b_i param: torch.Size([1500])\n",
      "param index: 4, name: rnn.lstm1.W_if param: torch.Size([1500, 1500])\n",
      "param index: 5, name: rnn.lstm1.W_hf param: torch.Size([1500, 1500])\n",
      "param index: 6, name: rnn.lstm1.b_f param: torch.Size([1500])\n",
      "param index: 7, name: rnn.lstm1.W_ig param: torch.Size([1500, 1500])\n",
      "param index: 8, name: rnn.lstm1.W_hg param: torch.Size([1500, 1500])\n",
      "param index: 9, name: rnn.lstm1.b_g param: torch.Size([1500])\n",
      "param index: 10, name: rnn.lstm1.W_io param: torch.Size([1500, 1500])\n",
      "param index: 11, name: rnn.lstm1.W_ho param: torch.Size([1500, 1500])\n",
      "param index: 12, name: rnn.lstm1.b_o param: torch.Size([1500])\n",
      "param index: 13, name: rnn.lstm2.W_ii param: torch.Size([1500, 1500])\n",
      "param index: 14, name: rnn.lstm2.W_hi param: torch.Size([1500, 1500])\n",
      "param index: 15, name: rnn.lstm2.b_i param: torch.Size([1500])\n",
      "param index: 16, name: rnn.lstm2.W_if param: torch.Size([1500, 1500])\n",
      "param index: 17, name: rnn.lstm2.W_hf param: torch.Size([1500, 1500])\n",
      "param index: 18, name: rnn.lstm2.b_f param: torch.Size([1500])\n",
      "param index: 19, name: rnn.lstm2.W_ig param: torch.Size([1500, 1500])\n",
      "param index: 20, name: rnn.lstm2.W_hg param: torch.Size([1500, 1500])\n",
      "param index: 21, name: rnn.lstm2.b_g param: torch.Size([1500])\n",
      "param index: 22, name: rnn.lstm2.W_io param: torch.Size([1500, 1500])\n",
      "param index: 23, name: rnn.lstm2.W_ho param: torch.Size([1500, 1500])\n",
      "param index: 24, name: rnn.lstm2.b_o param: torch.Size([1500])\n",
      "param index: 25, name: decoder.weight param: torch.Size([33278, 1500])\n",
      "param index: 26, name: decoder.bias param: torch.Size([33278])\n"
     ]
    }
   ],
   "source": [
    "for p_index, (name, param) in enumerate(vanilla_model.state_dict().items()):\n",
    "    print(\"param index: {}, name: {} param: {}\".format(p_index, name, param.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param index: 0, name: encoder.weight param: torch.Size([33278, 1500])\n",
      "param index: 1, name: rnn.lstm1.W_ii_U param: torch.Size([1500, 375])\n",
      "param index: 2, name: rnn.lstm1.W_ii_V param: torch.Size([375, 1500])\n",
      "param index: 3, name: rnn.lstm1.W_hi_U param: torch.Size([1500, 375])\n",
      "param index: 4, name: rnn.lstm1.W_hi_V param: torch.Size([375, 1500])\n",
      "param index: 5, name: rnn.lstm1.b_i param: torch.Size([1500])\n",
      "param index: 6, name: rnn.lstm1.W_if_U param: torch.Size([1500, 375])\n",
      "param index: 7, name: rnn.lstm1.W_if_V param: torch.Size([375, 1500])\n",
      "param index: 8, name: rnn.lstm1.W_hf_U param: torch.Size([1500, 375])\n",
      "param index: 9, name: rnn.lstm1.W_hf_V param: torch.Size([375, 1500])\n",
      "param index: 10, name: rnn.lstm1.b_f param: torch.Size([1500])\n",
      "param index: 11, name: rnn.lstm1.W_ig_U param: torch.Size([1500, 375])\n",
      "param index: 12, name: rnn.lstm1.W_ig_V param: torch.Size([375, 1500])\n",
      "param index: 13, name: rnn.lstm1.W_hg_U param: torch.Size([1500, 375])\n",
      "param index: 14, name: rnn.lstm1.W_hg_V param: torch.Size([375, 1500])\n",
      "param index: 15, name: rnn.lstm1.b_g param: torch.Size([1500])\n",
      "param index: 16, name: rnn.lstm1.W_io_U param: torch.Size([1500, 375])\n",
      "param index: 17, name: rnn.lstm1.W_io_V param: torch.Size([375, 1500])\n",
      "param index: 18, name: rnn.lstm1.W_ho_U param: torch.Size([1500, 375])\n",
      "param index: 19, name: rnn.lstm1.W_ho_V param: torch.Size([375, 1500])\n",
      "param index: 20, name: rnn.lstm1.b_o param: torch.Size([1500])\n",
      "param index: 21, name: rnn.lstm2.W_ii_U param: torch.Size([1500, 375])\n",
      "param index: 22, name: rnn.lstm2.W_ii_V param: torch.Size([375, 1500])\n",
      "param index: 23, name: rnn.lstm2.W_hi_U param: torch.Size([1500, 375])\n",
      "param index: 24, name: rnn.lstm2.W_hi_V param: torch.Size([375, 1500])\n",
      "param index: 25, name: rnn.lstm2.b_i param: torch.Size([1500])\n",
      "param index: 26, name: rnn.lstm2.W_if_U param: torch.Size([1500, 375])\n",
      "param index: 27, name: rnn.lstm2.W_if_V param: torch.Size([375, 1500])\n",
      "param index: 28, name: rnn.lstm2.W_hf_U param: torch.Size([1500, 375])\n",
      "param index: 29, name: rnn.lstm2.W_hf_V param: torch.Size([375, 1500])\n",
      "param index: 30, name: rnn.lstm2.b_f param: torch.Size([1500])\n",
      "param index: 31, name: rnn.lstm2.W_ig_U param: torch.Size([1500, 375])\n",
      "param index: 32, name: rnn.lstm2.W_ig_V param: torch.Size([375, 1500])\n",
      "param index: 33, name: rnn.lstm2.W_hg_U param: torch.Size([1500, 375])\n",
      "param index: 34, name: rnn.lstm2.W_hg_V param: torch.Size([375, 1500])\n",
      "param index: 35, name: rnn.lstm2.b_g param: torch.Size([1500])\n",
      "param index: 36, name: rnn.lstm2.W_io_U param: torch.Size([1500, 375])\n",
      "param index: 37, name: rnn.lstm2.W_io_V param: torch.Size([375, 1500])\n",
      "param index: 38, name: rnn.lstm2.W_ho_U param: torch.Size([1500, 375])\n",
      "param index: 39, name: rnn.lstm2.W_ho_V param: torch.Size([375, 1500])\n",
      "param index: 40, name: rnn.lstm2.b_o param: torch.Size([1500])\n",
      "param index: 41, name: decoder.weight param: torch.Size([33278, 1500])\n",
      "param index: 42, name: decoder.bias param: torch.Size([33278])\n"
     ]
    }
   ],
   "source": [
    "#for p_index, (name, param) in enumerate(low_rank_model.named_parameters()):\n",
    "#    print(\"param index: {}, name: {} param: {}\".format(p_index, name, param.size()))\n",
    "\n",
    "for p_index, (name, param) in enumerate(low_rank_model.state_dict().items()):\n",
    "    print(\"param index: {}, name: {} param: {}\".format(p_index, name, param.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_vanilla_model(vanilla_model, low_rank_model, rank_ratio=0.25):\n",
    "    collected_weights = []\n",
    "    for p_index, (name, param) in enumerate(vanilla_model.state_dict().items()):\n",
    "        if \"rnn.\" in name and len(param.size()) == 2:\n",
    "            rank = min(param.size()[0], param.size()[1])\n",
    "            sliced_rank = int(rank * rank_ratio)\n",
    "            u, s, v = torch.svd(param)\n",
    "            u_weight = u * torch.sqrt(s)\n",
    "            v_weight = torch.sqrt(s) * v\n",
    "            u_weight_sliced, v_weight_sliced = u_weight[:, 0:sliced_rank], v_weight[:, 0:sliced_rank]\n",
    "            collected_weights.append(u_weight_sliced)\n",
    "            collected_weights.append(v_weight_sliced.t())\n",
    "        else:\n",
    "            collected_weights.append(param)\n",
    "            \n",
    "    #for cw_index, cw in enumerate(collected_weights):\n",
    "    #     print(\"cw_index: {}, cw: {}\".format(cw_index, cw.size()))\n",
    "         \n",
    "    reconstructed_state_dict = {}\n",
    "    model_counter = 0\n",
    "    for p_index, (name, param) in enumerate(low_rank_model.state_dict().items()):\n",
    "        assert param.size() == collected_weights[model_counter].size()\n",
    "        reconstructed_state_dict[name] = collected_weights[model_counter]\n",
    "        model_counter += 1\n",
    "    low_rank_model.load_state_dict(reconstructed_state_dict)\n",
    "    return low_rank_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw_index: 0, cw: torch.Size([33278, 1500])\n",
      "cw_index: 1, cw: torch.Size([1500, 375])\n",
      "cw_index: 2, cw: torch.Size([375, 1500])\n",
      "cw_index: 3, cw: torch.Size([1500, 375])\n",
      "cw_index: 4, cw: torch.Size([375, 1500])\n",
      "cw_index: 5, cw: torch.Size([1500])\n",
      "cw_index: 6, cw: torch.Size([1500, 375])\n",
      "cw_index: 7, cw: torch.Size([375, 1500])\n",
      "cw_index: 8, cw: torch.Size([1500, 375])\n",
      "cw_index: 9, cw: torch.Size([375, 1500])\n",
      "cw_index: 10, cw: torch.Size([1500])\n",
      "cw_index: 11, cw: torch.Size([1500, 375])\n",
      "cw_index: 12, cw: torch.Size([375, 1500])\n",
      "cw_index: 13, cw: torch.Size([1500, 375])\n",
      "cw_index: 14, cw: torch.Size([375, 1500])\n",
      "cw_index: 15, cw: torch.Size([1500])\n",
      "cw_index: 16, cw: torch.Size([1500, 375])\n",
      "cw_index: 17, cw: torch.Size([375, 1500])\n",
      "cw_index: 18, cw: torch.Size([1500, 375])\n",
      "cw_index: 19, cw: torch.Size([375, 1500])\n",
      "cw_index: 20, cw: torch.Size([1500])\n",
      "cw_index: 21, cw: torch.Size([1500, 375])\n",
      "cw_index: 22, cw: torch.Size([375, 1500])\n",
      "cw_index: 23, cw: torch.Size([1500, 375])\n",
      "cw_index: 24, cw: torch.Size([375, 1500])\n",
      "cw_index: 25, cw: torch.Size([1500])\n",
      "cw_index: 26, cw: torch.Size([1500, 375])\n",
      "cw_index: 27, cw: torch.Size([375, 1500])\n",
      "cw_index: 28, cw: torch.Size([1500, 375])\n",
      "cw_index: 29, cw: torch.Size([375, 1500])\n",
      "cw_index: 30, cw: torch.Size([1500])\n",
      "cw_index: 31, cw: torch.Size([1500, 375])\n",
      "cw_index: 32, cw: torch.Size([375, 1500])\n",
      "cw_index: 33, cw: torch.Size([1500, 375])\n",
      "cw_index: 34, cw: torch.Size([375, 1500])\n",
      "cw_index: 35, cw: torch.Size([1500])\n",
      "cw_index: 36, cw: torch.Size([1500, 375])\n",
      "cw_index: 37, cw: torch.Size([375, 1500])\n",
      "cw_index: 38, cw: torch.Size([1500, 375])\n",
      "cw_index: 39, cw: torch.Size([375, 1500])\n",
      "cw_index: 40, cw: torch.Size([1500])\n",
      "cw_index: 41, cw: torch.Size([33278, 1500])\n",
      "cw_index: 42, cw: torch.Size([33278])\n"
     ]
    }
   ],
   "source": [
    "low_rank_model = decompose_vanilla_model(vanilla_model=vanilla_model, low_rank_model=low_rank_model, rank_ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 784)\n",
    "\n",
    "w = torch.randn(784, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = x @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 500])\n"
     ]
    }
   ],
   "source": [
    "print(o.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = torch.svd(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_rank = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_weight = u * torch.sqrt(s)\n",
    "v_weight = torch.sqrt(s) * v\n",
    "u_weight_sliced, v_weight_sliced = u_weight[:, 0:sliced_rank], v_weight[:, 0:sliced_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 500]) torch.Size([500, 500]) torch.Size([500]) torch.Size([500, 500]) torch.Size([500, 100]) torch.Size([500, 100])\n"
     ]
    }
   ],
   "source": [
    "print(w.size(), u.size(), s.size(), v.size(), u_weight_sliced.size(), v_weight_sliced.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(341.2319)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(w, torch.mm(u_weight_sliced, v_weight_sliced.t()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0008)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(w, torch.mm(torch.mm(u, torch.diag(s)), v.t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_flops(flops, rnn_module, w_ih, w_hh, input_size):\n",
    "    # matrix matrix mult ih state and internal state\n",
    "    flops += w_ih.shape[0]*w_ih.shape[1]\n",
    "    # matrix matrix mult hh state and internal state\n",
    "    flops += w_hh.shape[0]*w_hh.shape[1]\n",
    "    if isinstance(rnn_module, (nn.RNN, nn.RNNCell)):\n",
    "        # add both operations\n",
    "        flops += rnn_module.hidden_size\n",
    "    elif isinstance(rnn_module, (nn.GRU, nn.GRUCell)):\n",
    "        # hadamard of r\n",
    "        flops += rnn_module.hidden_size\n",
    "        # adding operations from both states\n",
    "        flops += rnn_module.hidden_size*3\n",
    "        # last two hadamard product and add\n",
    "        flops += rnn_module.hidden_size*3\n",
    "    elif isinstance(rnn_module, (nn.LSTM, nn.LSTMCell)):\n",
    "        # adding operations from both states\n",
    "        flops += rnn_module.hidden_size*4\n",
    "        # two hadamard product and add for C state\n",
    "        flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n",
    "        # final hadamard\n",
    "        flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_ih.shape[0]*w_ih.shape[1]\n",
    "# w_hh.shape[0]*w_hh.shape[1]\n",
    "flops = 0\n",
    "flops += (1500 * (4*1500) + (1500*4*1500))\n",
    "# flops += rnn_module.hidden_size*4\n",
    "flops += 1500 * 4\n",
    "#flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n",
    "flops += (1500 + 1500 + 1500)\n",
    "# flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n",
    "flops += (1500 + 1500 + 1500)\n",
    "# flops += b_ih.shape[0] + b_hh.shape[0]\n",
    "flops += (1500+1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18018000\n"
     ]
    }
   ],
   "source": [
    "print(flops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### low-rank LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_ih.shape[0]*w_ih.shape[1]\n",
    "# w_hh.shape[0]*w_hh.shape[1]\n",
    "flops = 0\n",
    "flops += (4*(1500*375+375*1500) + 4*(1500*375+375*1500))\n",
    "# flops += rnn_module.hidden_size*4\n",
    "flops += 1500 * 4\n",
    "#flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n",
    "flops += (1500 + 1500 + 1500)\n",
    "# flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n",
    "flops += (1500 + 1500 + 1500)\n",
    "# flops += b_ih.shape[0] + b_hh.shape[0]\n",
    "flops += (1500+1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9018000\n"
     ]
    }
   ],
   "source": [
    "print(flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
